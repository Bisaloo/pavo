---
title: "An introduction to pavo 1.0"
author: "Rafael Maia, Pierre-Paul Bitton, Chad Eliason, Thomas White"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
bibliography: refs.bib
vignette: >
  %\VignetteIndexEntry{An introduction to pavo 1.0}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
library(pavo)
```

# Introduction

`pavo` is an `R` package that was developed with the goal of establishing a flexible and integrated workflow for working with spectral color data. It includes functions that take advantage of new data classes to work seamlessly from importing raw data to visualization and analysis. Here we are excited to introduce the stable release of `pavo 1.0`, which---among other things---includes a suite of new analysis and visualisation tools to make working with colour data simpler and more intuitive than ever. 

`pavo` was written with the following workflow in mind, which remains true with this release:

1. **Organize** spectral data by inputting files and processing spectra (e.g., to remove noise, negative values, smooth curves, etc...).
2. **Analyze** the resulting files, either using typical colorimetric variables (hue, saturation, brightness) or using visual models based on perceptual data from the taxon of interest.
3. **Visualize** the output, with multiple options provided for exploratory analyses.

While we have made improvements that will expedite data organisation, the most significant developments in `1.0` relate to the analysis and visualisation of colour data. This includes greater flexibility in the initial visual modelling process, and a host of new colorspace models and associated plotting options which are accessable through a simplified workflow. Below we will provide an overview of the main new features introduced in `1.0` in an example workflow. 

# Data

`pavo 1.0` includes a new example dataset consisting of tidy reflectance spectra from 36 Australian angiosperm species. It can be accessed using `data(flowers)`, and we'll use it to illustrate many of the new features.   
```{r}
data(flowers)

head(flowers[1:4])
```

# Visual modelling

## Visual Phenotypes

`pavo` is now equipped with several new di-, tri- and tetrachromatic visual systems, to accompany the suite of new models. The full complement of included systems are accessable via the `vismodel()` option `visual`, and include:
  : `avg.uv` average ultraviolet-sensitive avian (tetrachromat)
  : `avg.v` average violet-sensitive avian (tetrachromat)
  : `bluetit` The blue tit _Cyanistes caeruleus_ (tetrachromat)
  : `star` The starling _Sturnus vulgaris_ (tetrachromat)
  : `pfowl` The peafowl _Pavo cristatus_ (tetrachromat)
  : `apis` The honeybee _Apis mellifera_ (trichromat)
  : `canis` The canid _Canis familiaris_ (dichromat)
  : `musca` The housefly _Musca domestica_ (tetrachromat)
  : `cie2` 2-degree colour matching functions for CIE models of human colour vision (trichromat)
  : `cie10` 10-degree colour matching functions for CIE models of human colour vision (trichromat)

As in previous versions, the individual sensitivity functions can be accessed via a call to `vissyst`, and visualised using `plot`. For example:

```{r fig=TRUE, include=TRUE, fig.width=7.2, fig.height=5, fig.align='center', fig.cap="_The visual sensitivities of the muscoid fly Musca domestica._"}

plot(as.rspec(vissyst[, c('wl', 'musca.u', 'musca.s', 'musca.m', 'musca.l', 'md.r1')]), main = 'Musca domestica', ylab = 'Absorbance')

```

## The `vismodel` Function and Receptor-noise Limited Model

...new tweaks to vismodel & receptor noise (raf?)...

...`vismodel` also now includes the option of hyperbolically transforming quantum catches by specifying `qcatch = 'Ei'`:

$$E_i = \frac{Q_i}{Q_i + 1}$$

Where E~i~ is the 'excitation value` for photoreceptor _i_, and Q~i~ is the raw quantum catch of said photoreceptor. The transformation is a simplification of the Michaelisâ€“Menton photoreceptor equation [@dowling1987retina; @backhaus1987color], and is particularly common in models of Hymenopteran vision [@backhaus1987color; @chittka1992colour; @backhaus1991color].  

## Modelling and Plotting Colourspaces

### The `colspace` and `plot` Functions

The most significant structural changes in `1.0` involve the workflow for visual modelling, particular as regards the use of colorspaces. Previously, for example, we might use `vismodel` to model photoreceptor stimulation to an avian viewer, `tcs` to convert the data to points in a tetrahedral colorspace, and `tcsplot` to visualise the data. In `1.0` these, and all other, colorspace modelling and plotting options are now wrapped into the new `colspace`  and `plot` functions. For modelling in colorspace, a typical workflow might use:  

1. `vismodel` to estimate photoreceptor quantum catches. The assumptions of many colorspace models may differ dramatically, so be sure to select options that are appropriate for your intended use.   
2. `colspace` to convert the results of `vismodel` (or user-provided quantum catches) into points in a given colorspace, specified with the `space` argument. If no `space` argument is provided, `colspace` will automatically select the di-, tri- or tetrahedral colorspace, depending on the nature of the input data. The result of `colspace` will be an object of class `colspace` (that inherits from `data.frame`; see the 'Under-the-hood' section below), and will contain the location of stimuli in the selected space along with any associated color variables. 
3. `plot` the output. `plot` will automatically select the appropriate visualisation based on the input `colspace` object, and will also accept various graphical parameters depending on the colorspace (see `?plot.colspace` and links therein for details) . 

### Di-, Tri-, and Tetrachromatic Spaces

While `pavo` has always included the ability to model spectra in a tetrahedral colourspace, `1.0` now expands this ability to di- and trichromatic spaces, and (as outlined above) unites these approaches in a cohesive workflow. As with most colorspace models, we first estimate relative quantum catches with various assumptions by using the `vismodel` function, before converting each set of values to a location in colorspace by using the `space` argument in `colspace`. For di- tri- and tetrechromatic spaces, `colspace` calculates the coordinates of stimuli as:
  : Dichromats:
    : $$x = \frac{1}{\sqrt{2}}(Q_l - Q_s)$$
  : Trichromats:
    : $$x = \frac{1}{\sqrt{2}}(Q_l - Q_m)$$
    : $$y = \frac{\sqrt{2}}{\sqrt{3}}(Q_s - \frac{Q_l + Q_m}{2})$$
  : Tetrachromats:
    : $$x = \frac{1}{\sqrt{2}}(Q_l - Q_m)$$
    : $$y = \frac{\sqrt{2}}{\sqrt{3}}(Q_s - \frac{Q_l + Q_m}{2})$$
    : $$z = \frac{\sqrt{3}}{2}(Q_u - \frac{Q_l + Q_m + Q_s}{3})$$

Where Q~u~, Q~s~, Q~m~, and Q~l refer to quantum catch estimates for UV-, short, medium-, and long-wavelength photoreceptors.

For a dichromatic example, we can model our floral reflectance data using the visual system of the domestic dog _Canis familiaris_, which has two cones with maximal sensitivity near 440 and 560 nm.   

```{r}
vis.flowers <- vismodel(flowers, visual = 'canis')

di.flowers <- colspace(vis.flowers, space = 'di')

head(di.flowers)
```

The output contains values for the relative stimulation of shot- and long-wavelength sensitive photoreceptors associated with each flower, along with its single coordinate in dichromatic space and its r.vector (distance from the origin). To visualise where these points lie, we can simply plot them on a segment.

```{r, fig=TRUE, include=TRUE, fig.width=5, fig.height=5, fig.align='center', fig.cap="_Flowers in a dichromatic colorspace, as modelled according to a canid visual system._"}
plot(di.flowers, pch = 21, bg = 'forestgreen') 
```

For our trichromatic viewer we can use the honeybee _Apis melifera_, one of the most significant and widespread pollinators. We'll also transform our quantum catches according to Fechner's law by specifying `qcatch = 'fi'`, and will model photoreceptor stimulation under bright conditions by scaling our illuminant with the `scale` argument. 

```{r}
vis.flowers <- vismodel(flowers, visual = 'apis', qcatch = 'fi', scale = 10000)

tri.flowers <- colspace(vis.flowers, space = 'tri')

head(tri.flowers)
```

As in the case of our dichromat, the output contains relative photoreceptor stimulations, coordinates in the Maxwell triangle, a well as the 'hue angle' `h.theta` and distance from the origin (`r.vec`).  

```{r, fig=TRUE, include=TRUE, fig.width=6, fig.height=6, fig.align='center', fig.cap="_Floral reflectance in a Maxwell triangle, considering a honeybee visual system._"}
plot(tri.flowers, pch = 21, bg = 'forestgreen') 
```

Finally, we'll draw on the blue tit's visual system to model our floral reflectance spectra in a tetrahedral space, again using log-transformed quantum catches and assuming bright viewing conditions. 

```{r}
vis.flowers <- vismodel(flowers, visual = 'bluetit', qcatch = 'fi', scale = 10000)

tetra.flowers <- colspace(vis.flowers, space = 'tcs')

head(tetra.flowers)
```

As in previous versions of `pavo`, tetrahedral data (now via `colspace(space = 'tcs')`) may be visualised in an _interactive_ plot by using `tcsplot`, along with the accessory functions `tcspoints` and `tcsvol` for adding points and convex hulls, respectively. Version `1.0`, however, now also includes a _static_ tetrahedral plot, based on functionality from the package `scatterplot3d`. As with other colorspace plots there are a number of associated graphical options, though the `view` option is particularly useful in this case, as it controls the orientation of the tetrahedron by specifying a viewing angle from 0 to 360 (in degrees).

```{r, fig=TRUE, include=TRUE, fig.width=7.2, fig.height=5, fig.align='center', fig.cap="_Flowers in a tetrahedral colorspace, with varied orientations, modelled using the visual phenotype of the blue tit._"}
par(mfrow = c(1, 2))
plot(tetra.flowers, view = 100, pch = 21, bg = 'forestgreen') 
plot(tetra.flowers, view = 75, pch = 21, bg = 'forestgreen')
```

### The Color Hexagon

The hexagon colour space of Chittka [-@chittka1992colour] is a model of hymenopteran vision that has found extremely broad use, particularly in studies of bee-flower interactions. It's also often broadly applied across hymenopteran species, because the photopigments underlying trichromatic vision in Hymenoptera appear to be quite conserved [@briscoe2001evolution]. What's particularly useful is that color distances within the hexagon have been extensively validated against behaviour, and thus offer a relatively reliable measure of perceptual distance [e.g. @dyer2008comparative; @avargues2010aversive; @chittka1992colour].

In the hexagon, photoreceptor quantum catches are typically hyperbolically transformed (and `pavo` will return a warning if the transform is not selected), and vonkries correction is often used used to model photoreceptor adaptation to a vegetation background. This can all now be specified in `vismodel`. including the optional use of a 'green' vegetation background. Note that although this is a colorspace model, we specific `relative = FALSE` to return raw (albeit transformed) quantum catches, as required for the model [@chittka1992colour]. 

```{r}
vis.flowers <- vismodel(flowers, visual = 'apis', qcatch = 'Ei', relative = FALSE, vonkries = TRUE, achro = 'l', bkg = 'green')
```

We can then apply the hexagon model in `colspace`, which will convert our photoreceptor 'excitation values' to coordinates in the hexagon according to:

$$x = \frac{\sqrt{3}}{2(E_g = E_{uv})}$$

$$y = E_b - 0.5(E_{uv} + E_g)$$

```{r}
hex.flowers <- colspace(vis.flowers, space = 'hexagon')

head(hex.flowers)
```

Again, the output includes the photoreceptor excitation values for short- medium- and long-wave sensitive photoreceptors, x and y coordinates, and measures of hue and saturation for each stimulus. The hegaon model also outputs two additoinal measures of of subjective 'bee-hue'; `sec.fine` and `sec.coarse`. `sec.fine` describes the location of stimuli within one of 36 'hue sectors' that are specified by radially dissecting the hexagon in 10-degree increments. `sec.coarse` follows a similar principle, though here the hexagon is divided into only five 'bee-hue' sectors: UV, UV-blue, blue, blue-green, green, and UV-green [@ultraviolet1994chittka; @dyer2012parallel]. These can easily be visualised by specifying `sectors = 'coarse` or `sectors = 'fine'` in a call to `plot` after modelling.  

```{r, fig=TRUE, include=TRUE, fig.width=6, fig.height=6, fig.align='center', fig.cap="_Flowers as modelled in the hymenopteran colour hexagon of Chittka (1992), overlain with coarse bee-hue sectors._"}
plot(hex.flowers, sectors = 'coarse', pch = 21, bg = 'forestgreen')
```

### The Color Opponent Coding (COC) Space

The color opponent coding (coc) space is an earlier hymenopteran visual model [@backhaus1991color]. While the initial estimation of photoreceptor excitation is similar to that in the hexagon, the coc subsequently specifies `A` and `B` coordinates based on empirically-derived weights for the output from each photoreceptor:

$$A = -9.86E_g + 7.70E_b + 2.16E_g$$
$$B = -5.17E_g + 20.25E_b - 15.08E_g$$

Where E~i~ is the excitation value (quantum catch) in photoreceptor _i_.

```{r}
vis.flowers <- vismodel(flowers, visual = 'apis', qcatch = 'Ei', relative = FALSE, vonkries = TRUE, bkg = 'green')

coc.flowers <- colspace(vis.flowers, space = 'coc')

head(coc.flowers)

```

The A and B coordinates are designated x and y in the output of coc for consistency, and while the model includes a measure of saturation in `r.vec`, it contains no associated measure of hue.

```{r, fig=TRUE, include=TRUE, fig.width=6, fig.height=6, fig.align='center', fig.cap="_Flowers in the color-opponent-coding space of Backhaus (1991), as modelling according to the honeybee._"}
plot(coc.flowers, pch = 21, bg = 'forestgreen') 
```

### CIE Spaces

The CIE (International Commission on Illumination) colorspaces are a suite of models of human color vision and perception. `pavo 1.0` now includes two of the most commonly used: the foundational 1931 CIE XYZ space, and the more modern, perceptually calibrated CIE LAB space. Tristimulus values in XYZ space are calculated as:

$$X = k\int_{300}^{700}{R(\lambda)I(\lambda)\bar{x}(\lambda)d\lambda}$$
$$Y = k\int_{300}^{700}{R(\lambda)I(\lambda)\bar{y}(\lambda)d\lambda}$$
$$Z = k\int_{300}^{700}{R(\lambda)I(\lambda)\bar{z}(\lambda)d\lambda}$$

where _x_, _y_, and _z_ are the trichromatic color matching functions for a 'standard colorimetric viewer'. These functions are designed to describe an average human's chromatic response within a specified viewing arc in the fovea (to account for the uneven distribution of cones across eye). `pavo` includes both the CIE 2-degree and the modern 10-degree standard observer, which can be selected in the `visual` option in the `vismodel` function. In these equations, k is the normalising factor

$$k = \frac{100}{\int_{300}^{700}{I(\lambda)\bar{y}(\lambda)d\lambda}}$$

and the chromaticity coordinates of stimuli are calculated as

$$x = \frac{X}{X + Y + Z}$$
$$y = \frac{Y}{X + Y + Z}$$
$$z = \frac{Z}{X + Y + Z} = 1 - x - y$$

For modelling in both XYZ and LAB spaces, here we'll use the CIE 10-degree standard observer, and assume a `D65` 'standard daylight' illuminant. Again, although they are colorspace models, we need to set `relative = FALSE` to return raw quantum catch estimates, `vonkries = TRUE` to include for the required normalising factor (as above), and `achromatic = 'none'` since there is no applicable way to estimate luminance in the CIE models. As with all models that have particular requirements, `vismodel` will output warnings and/or errors if unusual or non-standard arguments are specified.

```{r}
vis.flowers <- vismodel(flowers, visual = 'cie10', illum = 'D65', vonkries = TRUE, relative = FALSE, achromatic = 'none')
```

```{r}
ciexyz.flowers <- colspace(vis.flowers, space = 'ciexyz')
head(ciexyz.flowers)
```

The output is simply the tristimulus values and chromaticity coordinates of stimuli, and we can visualise our results (along with a line connecting monochromatic loci, by default) by calling 

```{r, fig=TRUE, include=TRUE, fig.width=6, fig.height=6, fig.align='center', fig.cap="_Floral reflectance in the CIEXYZ human visual model. Note that this space is not perceptually calibrated, so we cannot make inferences about the similarity or differences of colours based on their relative location._"}
plot(ciexyz.flowers, pch = 21, bg = 'forestgreen') 
```

The `Lab` space is a more recent development, and is a color-opponent model that attempts to model the nonlinear responses of the human eye. The `Lab` model is also calibrated (unlike the `XYZ` model) such that Euclidean distances between points represent relative perceptual distances. This means that two stimuli that are farther apart in `Lab` space should be _percieved_ as more different that two closer points. As the name suggests, the dimensions of the `Lab` space are *L*ightness (i.e. subjective brightness), along with two color-opponent dimensions designated *a* and *b*. The `colspace` function, when `space = cielab`, simply converts points from the `XYZ` model according to:

$$L = 116\left(\frac{Y}{Y_n}\right)^\frac{1}{3} \;\; if \;\; \frac{Y}{Y_n} > 0.008856$$
$$L = 903.3\left(\frac{Y}{Y_n}\right) \;\; if \;\; \frac{Y}{Y_n} \leq 0.008856$$

$$a = 500\left(f\left(\frac{X}{X_n}\right) - f\left(\frac{Y}{Y_n}\right)\right)$$

$$b = 500\left(f\left(\frac{Y}{Y_n}\right) - f\left(\frac{Z}{Z_n}\right)\right)$$

where

$$f(x) = x^\frac{1}{3} \;\; if \;\; x > 0.008856$$
$$f(x) = 7.787(x) + \frac{16}{116} \;\; if \;\; x \leq 0.008856$$

Here, $X_n, Y_n, Z_n$ are neutral point values to model visual adaptation, calculated as:

$$X_n = \int_{300}^{700}{R_n(\lambda)I(\lambda)\bar{x}(\lambda)d\lambda}$$
$$Y_n = \int_{300}^{700}{R_n(\lambda)I(\lambda)\bar{y}(\lambda)d\lambda}$$
$$Z_n = \int_{300}^{700}{R_n(\lambda)I(\lambda)\bar{z}(\lambda)d\lambda}$$

when $R_n(\lambda)$ is a perfect diffuse reflector (i.e. 1).

```{r}

cielab.flowers <- colspace(vis.flowers, space = 'cielab')
head(cielab.flowers)

```

Our output now contains the tristimulus `XYZ` values, as well as their `Lab` counterparts, which are coordinates in the `Lab` space. These can also be visualised in three-dimensional `Lab` space by calling `plot`: 

```{r, fig=TRUE, include=TRUE, fig.width=6, fig.height=6, fig.align='center', fig.cap="_CIELAB._"}
plot(cielab.flowers, pch = 21, bg = 'forestgreen') 
```

### Categorical Fly Colourspace

The categorical colour vision model of Troje [-@troje1993spectral] is a model of dipteran vision, based on behavioural data from the blowfly _Lucilia_ sp. It assumes the involvement of all four dipteran photoreceptor classes (R7 & R8 'pale' and 'yellow' subtypes), and further posits that colour vision is based on two specific opponent mechanisms (R7p - R8p, and R7y - R8y). The model assumes that all colours are perceptually grouped into one of four colour categories, and that flies are unable to distinguish between colours that fall within the same category.

We'll use the visual systems of the mudcoid fly _Musca domestica_, and will begin by estimating linear (i.e. untransformed) quantum catches for each of the four photoreceptors [@troje1993spectral].

```{r}
vis.flowers <- vismodel(flowers, qcatch = 'Qi', visual = 'musca', achro = 'none', relative = TRUE)
```

Our call to `colspace` will then simply estimate the location of stimuli in the categorical space as the difference in relative stimulation between 'pale' (R7p - R8p) and 'yellow' (R7y - R8y) photoreceptor pairs:

$$x = R7_p - R8_p$$

$$y = R7_y - R8_y$$

```{r}
cat.flowers <- colspace(vis.flowers, space = 'categorical')

head(cat.flowers)
```

And it is simply the signs of these differences that define the four possible fly-colour categories (p+y+, p-y+, p+y-, p-y-), which we can see in the associated plot.

```{r, fig=TRUE, include=TRUE, fig.width=6, fig.height=6, fig.align='center', fig.cap="_Flowers in the categorical colorspace of Troje (1993)._"}
plot(cat.flowers, pch = 21, bg = 'forestgreen') 
```

# Color Distances in N-dimensions with `coldist`

Raf?

# Under-the-Hood

## Classes and Attributes

# Help

For assistance and/or bug reports, we suggest getting in touch via 'gitter' at [https://gitter.im/r-pavo/help](https://gitter.im/r-pavo/help), which is essentially a public chat room for all things pavo. If you have a bug to report, we'd appreciate it if you could also include a reproducible example when possible.

# References


